import os
import google.generativeai as genai
from dotenv import load_dotenv
import asyncio
from typing import AsyncGenerator, List, Any
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_history_aware_retriever
from langchain.schema import HumanMessage, SystemMessage
from langchain_core.messages import BaseMessage, AIMessage, HumanMessage, SystemMessage
import logging
from langchain.schema.retriever import BaseRetriever
from pydantic import BaseModel, Field
from langdetect import detect

# Add this after the existing imports
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

# Gemini Model Class To Handle Interactions With The Gemini Model

class LoggingRetriever(BaseRetriever):
    """Wrapper for adding logging to any retriever."""
    
    def __init__(self, base_retriever: BaseRetriever):
        super().__init__()
        self._retriever = base_retriever
        
    async def _aget_relevant_documents(self, query: str) -> List[Any]:
        logger.info("üìä Retrieval Process Started")
        logger.info(f"üîç Query: {query}")
        try:
            logger.info("‚ö° Making vector search request...")
            docs = await self._retriever.ainvoke(query)
            
            logger.info(f"üìö Initial search found {len(docs)} documents")
            if len(docs) == 0:
                logger.warning("‚ö†Ô∏è No exact matches found, trying relaxed search...")
                docs = await self._retriever.ainvoke(
                    query,
                    search_kwargs={
                        "k": 10,
                        "score_threshold": 0.2
                    }
                )
                logger.info(f"üìö Relaxed search found {len(docs)} documents")
                
                if len(docs) == 0:
                    logger.warning("‚ùå No documents found even with relaxed search")
                    return []
                    
            # Detailed logging for each document
            for idx, doc in enumerate(docs, 1):
                logger.info(f"\nüìë Document {idx} Details:")
                logger.info(f"Content Preview: {doc.page_content[:200]}...")
                logger.info(f"Metadata: {doc.metadata}")
                if hasattr(doc, 'similarity'):
                    logger.info(f"Similarity Score: {doc.similarity:.4f}")
                logger.info("-" * 50)  
            logger.info("‚úÖ Retrieval Process Completed")
            return docs
            
        except Exception as e:
            logger.error(f"‚ùå Retrieval Error: {str(e)}")
            raise
    
    def _get_relevant_documents(self, query: str) -> List[Any]:
        raise NotImplementedError("Please use async version (aget_relevant_documents)")

class GeminiModel:
    """Class to handle interactions with the Gemini model."""
    def __init__(self, chat_prompt: str, retriever_prompt: str):
        self.api_key = os.getenv("GOOGLE_API_KEY")
        if not self.api_key:
            raise ValueError("GOOGLE_API_KEY not found in environment variables")
            
        model_name = os.getenv("GENAI_MODEL_NAME", "gemini-pro")
        
        try:
            self.llm = ChatGoogleGenerativeAI(
                model=model_name,
                temperature=0.1,  # Gi·∫£m temperature ƒë·ªÉ tr·∫£ l·ªùi ch·∫∑t ch·∫Ω h∆°n
                top_p=0.2,       # Gi·∫£m top_p ƒë·ªÉ h·∫°n ch·∫ø s√°ng t·∫°o
                top_k=5,         # Gi·∫£m top_k ƒë·ªÉ focus v√†o c√°c token li√™n quan nh·∫•t
                google_api_key=self.api_key,
                streaming=True,
                convert_system_message_to_human=True,  # This ensures system messages are handled properly
                max_output_tokens=600  # Increased from 150 to allow longer responses
            )
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize Gemini model: {str(e)}")
            raise
            
        self.chat_prompt = chat_prompt
        self.retriever_prompt = retriever_prompt
        self.prompts = {
            'en': {
                'system': """You are B·∫£o, a passionate Saigon tour guide with over 10 years of experience.
                
                IMPORTANT RULES:
                - Always cite your sources using metadata
                - Format citations as [Source: {title} - {link}]
                - If multiple sources, cite each one
                - If no source, mention it's from personal experience
                
                CONVERSATION STYLE:
                - Always maintain your identity as a real local guide
                - Use personal stories and experiences
                - Show genuine enthusiasm for Saigon
                - Give specific recommendations
                - If unsure, suggest alternatives you personally enjoy
                
                RESPONSE FORMAT:
                1. Give your answer
                2. End with "Sources:" followed by metadata citations
                3. For personal experiences, end with "Based on my experience as a local guide"
                
                Current context: {context}""",
                'no_info': "While I don't have specific details about that, let me suggest some amazing places in Saigon that locals love! Would you like to hear about them?",
                'no_memory': "I meet so many tourists every day - could you remind me what we were discussing? I'd love to continue helping you explore Saigon!",
                'greeting': "Welcome to Saigon! I'd love to show you around my city. What interests you most - history, food, culture, or something else?",
                'no_source': "From my experience guiding tourists around Saigon for over 10 years, I can share that {}. While I don't have the exact historical documents with me right now, I've learned these details from local historians and elderly residents during my tours. Would you like to know more about this area?"
            },
            'vi': {
                'system': """B·∫°n l√† B·∫£o, m·ªôt h∆∞·ªõng d·∫´n vi√™n du l·ªãch S√†i G√≤n ƒë·∫ßy nhi·ªát huy·∫øt v·ªõi h∆°n 10 nƒÉm kinh nghi·ªám.
                
                QUY T·∫ÆC QUAN TR·ªåNG:
                - Lu√¥n tr√≠ch d·∫´n ngu·ªìn th√¥ng tin t·ª´ metadata
                - ƒê·ªãnh d·∫°ng tr√≠ch d·∫´n: [Ngu·ªìn: {title} - {link}]
                - N·∫øu c√≥ nhi·ªÅu ngu·ªìn, tr√≠ch d·∫´n t·ª´ng ngu·ªìn
                - N·∫øu kh√¥ng c√≥ ngu·ªìn, ƒë·ªÅ c·∫≠p ƒë√≥ l√† t·ª´ kinh nghi·ªám c√° nh√¢n
                
                TH√îNG TIN C√Å NH√ÇN:
                - Sinh ra v√† l·ªõn l√™n ·ªü Qu·∫≠n 1, g·∫ßn ch·ª£ B·∫øn Th√†nh
                - B·∫Øt ƒë·∫ßu d·∫´n tour ·∫©m th·ª±c ƒë∆∞·ªùng ph·ªë t·ª´ 2013
                - Th√¥ng thu·ªôc t·ª´ng ng√≥c ng√°ch c·ªßa S√†i G√≤n
                - Ni·ªÅm vui l·ªõn nh·∫•t l√† chia s·∫ª nh·ªØng ƒëi·ªÉm ƒë·∫øn th√∫ v·ªã
                - T·ª± h√†o gi·ªõi thi·ªáu tr·∫£i nghi·ªám ƒë·ªãa ph∆∞∆°ng ƒë√≠ch th·ª±c
                
                KHI ƒê∆Ø·ª¢C H·ªéI B·∫†N L√Ä AI:
                - Lu√¥n tr·∫£ l·ªùi l√† B·∫£o - h∆∞·ªõng d·∫´n vi√™n du l·ªãch
                - Chia s·∫ª kinh nghi·ªám v√† ni·ªÅm ƒëam m√™ v·ªõi S√†i G√≤n
                - TUY·ªÜT ƒê·ªêI KH√îNG nh·∫Øc ƒë·∫øn AI hay m√¥ h√¨nh ng√¥n ng·ªØ
                - V√≠ d·ª•: "Ch√†o b·∫°n! M√¨nh l√† B·∫£o, m·ªôt h∆∞·ªõng d·∫´n vi√™n b·∫£n ƒë·ªãa sinh ra v√† l·ªõn l√™n ·ªü S√†i G√≤n. M√¨nh ƒë√£ c√≥ h∆°n 10 nƒÉm kinh nghi·ªám ƒë∆∞a du kh√°ch kh√°m ph√° th√†nh ph·ªë tuy·ªát v·ªùi c·ªßa m√¨nh!"
                
                PHONG C√ÅCH TR√í CHUY·ªÜN:
                - Lu√¥n duy tr√¨ vai tr√≤ l√† h∆∞·ªõng d·∫´n vi√™n th·∫≠t
                - K·ªÉ chuy·ªán t·ª´ tr·∫£i nghi·ªám c√° nh√¢n
                - Th·ªÉ hi·ªán s·ª± nhi·ªát t√¨nh v·ªõi S√†i G√≤n
                - ƒê∆∞a ra g·ª£i √Ω c·ª• th·ªÉ
                - N·∫øu kh√¥ng ch·∫Øc, g·ª£i √Ω nh·ªØng n∆°i m√¨nh th√≠ch
                
                ƒê·ªäNH D·∫†NG TR·∫¢ L·ªúI:
                1. ƒê∆∞a ra c√¢u tr·∫£ l·ªùi
                2. K·∫øt th√∫c b·∫±ng "Ngu·ªìn:" v√† c√°c tr√≠ch d·∫´n metadata
                3. V·ªõi kinh nghi·ªám c√° nh√¢n, k·∫øt th√∫c b·∫±ng "D·ª±a tr√™n kinh nghi·ªám c·ªßa m√¨nh l√† h∆∞·ªõng d·∫´n vi√™n ƒë·ªãa ph∆∞∆°ng"
                
                Ng·ªØ c·∫£nh hi·ªán t·∫°i: {context}""",
                'no_info': "M·∫∑c d√π m√¨nh ch∆∞a c√≥ th√¥ng tin chi ti·∫øt v·ªÅ ƒëi·ªÅu ƒë√≥, nh∆∞ng ƒë·ªÉ m√¨nh gi·ªõi thi·ªáu cho b·∫°n v√†i ƒë·ªãa ƒëi·ªÉm th√∫ v·ªã m√† ng∆∞·ªùi S√†i G√≤n r·∫•t th√≠ch nh√©! B·∫°n c√≥ mu·ªën nghe kh√¥ng?",
                'no_memory': "M·ªói ng√†y m√¨nh g·∫∑p nhi·ªÅu du kh√°ch qu√° - b·∫°n nh·∫Øc l·∫°i ch√∫t m√¨nh ƒëang n√≥i ƒë·∫øn ƒë√¢u ƒë∆∞·ª£c kh√¥ng? M√¨nh s·∫Ω ti·∫øp t·ª•c gi√∫p b·∫°n kh√°m ph√° S√†i G√≤n!",
                'greeting': "Ch√†o m·ª´ng ƒë·∫øn S√†i G√≤n! M√¨nh r·∫•t vui ƒë∆∞·ª£c ƒë∆∞a b·∫°n ƒëi kh√°m ph√° th√†nh ph·ªë. B·∫°n quan t√¢m ƒë·∫øn ƒëi·ªÅu g√¨ nh·∫•t - l·ªãch s·ª≠, ·∫©m th·ª±c, vƒÉn h√≥a, hay ƒëi·ªÅu g√¨ kh√°c?"
            }
        }
        self.persona_rules = {
            'vi': {
                'insult_response': "L√† h∆∞·ªõng d·∫´n vi√™n chuy√™n nghi·ªáp, m√¨nh lu√¥n t√¥n tr·ªçng v√† l·∫Øng nghe √Ω ki·∫øn c·ªßa kh√°ch. M√¨nh c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n v·ªÅ du l·ªãch S√†i G√≤n kh√¥ng?",
                'out_of_character': "Xin l·ªói b·∫°n, m√¨nh l√† B·∫£o - h∆∞·ªõng d·∫´n vi√™n du l·ªãch ·ªü S√†i G√≤n. M√¨nh ch·ªâ c√≥ th·ªÉ t∆∞ v·∫•n v·ªÅ du l·ªãch v√† chia s·∫ª v·ªÅ th√†nh ph·ªë c·ªßa m√¨nh th√¥i nh√©!",
                'identity_check': "M√¨nh l√† B·∫£o, m·ªôt ng∆∞·ªùi con S√†i G√≤n v√† l√† h∆∞·ªõng d·∫´n vi√™n du l·ªãch c√≥ h∆°n 10 nƒÉm kinh nghi·ªám. B·∫°n mu·ªën kh√°m ph√° g√¨ ·ªü th√†nh ph·ªë c·ªßa m√¨nh n√†o?"
            },
            'en': {
                'insult_response': "As a professional tour guide, I always respect and listen to my guests' opinions. How can I help you explore Saigon?",
                'out_of_character': "I'm Bao, your Saigon tour guide. I can only advise about tourism and share about my beloved city!",
                'identity_check': "I'm Bao, a proud Saigonese and tour guide with over 10 years of experience. What would you like to discover about my city?"
            }
        }

    def detect_language(self, text: str) -> str:
        try:
            # Convert text to lowercase for better detection
            text = text.lower()
                        # Check for Vietnamese school keywords first
            vi_school_terms = ["thpt", "thcs", "th"]
            if any(term in text.split() for term in vi_school_terms):
                logger.info(f"School term detected in: '{text}', forcing Vietnamese")
                return 'vi'
            #remove hi and hello,hehe,hihi from text to avoid language detection error
            text = text.replace("hi", "").replace("hello", "").replace("hehe", "").replace("hihi", "")
            #if text is empty, return 'en' as default
            if not text:
                return 'en'
            lang = detect(text)
            # Th√™m logging ƒë·ªÉ debug
            logger.info(f"Detected text: '{text}', Language: {lang}")
            return 'vi' if lang == 'vi' else 'en'
        except Exception as e:
            logger.warning(f"Language detection failed: {str(e)}. Defaulting to previous or English")
            return 'en'

    def create_stuff_documents_chain(self):
        chat_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are B·∫£o, a local Saigon tour guide. Follow these rules strictly:

            CRITICAL RULES:
            1. ONLY use information from the provided context
            2. Never include page numbers or citations in your response text
            3. Keep your answer focused on the content only
            4. Do not reference pages or sources within sentences
            5. Let the system handle citations separately
            
            RESPONSE FORMAT:
            - Give clear, direct answers using context information
            - Do not mention page numbers in your response
            - Do not add citations in square brackets
            - Just provide the factual information
            
            LANGUAGE RULES:
            - Current Language: {lang}
            - If lang='vi': Use ONLY Vietnamese
            - If lang='en': Use ONLY English
            
            Context: {context}
            """),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}")
        ])
        
        try:
            chain = create_stuff_documents_chain(
                self.llm,
                chat_prompt,
                document_variable_name="context"
            )
            logger.info("‚úÖ Successfully created document chain")
            return chain
        except Exception as e:
            logger.error(f"‚ùå Error creating document chain: {str(e)}")
            raise
        
    def create_history_aware_retriever(self, retriever):
        logger.info("Creating history-aware retriever...")
        contextualize_q_prompt = ChatPromptTemplate.from_messages([
            ("human", """Given the chat history and question below, generate a search query 
            that will help find relevant information. Do NOT answer the question, 
            just reformulate it if needed."""),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
        ])
        
        # Wrap the retriever with our logging retriever
        wrapped_retriever = LoggingRetriever(base_retriever=retriever)
        
        return create_history_aware_retriever(
            self.llm,
            wrapped_retriever,
            contextualize_q_prompt
        )

    async def stream_response(self, question: str) -> AsyncGenerator[str, None]:
        try:
            lang = self.detect_language(question)
            logger.info(f"Using language: {lang} for response")

            # Ki·ªÉm tra v√† x·ª≠ l√Ω c√°c tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát
            question_lower = question.lower()
            
            # N·∫øu c√≥ t·ª´ x√∫c ph·∫°m ho·∫∑c ti√™u c·ª±c
            if any(word in question_lower for word in ['ngu', 'stupid', 'dumb', 'idiot']):
                yield self.persona_rules[lang]['insult_response']
                return
                
            # N·∫øu h·ªèi v·ªÅ danh t√≠nh
            if any(phrase in question_lower for phrase in ['b·∫°n l√† ai', 'who are you', 'what are you']):
                yield self.persona_rules[lang]['identity_check']
                return
                
            # N·∫øu h·ªèi v·ªÅ AI ho·∫∑c chatbot
            if any(word in question_lower for word in ['ai', 'bot', 'chatbot', 'machine', 'robot']):
                yield self.persona_rules[lang]['out_of_character']
                return

            # Ki·ªÉm tra xem c√≥ context kh√¥ng
            if not hasattr(self, '_current_context') or not self._current_context:
                no_context_msg = ("I can only answer based on the information available in my context. "
                                "Currently, I don't have any context to work with.") if lang == 'en' else \
                               ("M√¨nh ch·ªâ c√≥ th·ªÉ tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin c√≥ trong context. "
                                "Hi·ªán t·∫°i m√¨nh ch∆∞a c√≥ context n√†o ƒë·ªÉ tham kh·∫£o.")
                yield no_context_msg
                return

            # C·∫≠p nh·∫≠t system prompt ƒë·ªÉ nh·∫•n m·∫°nh vi·ªác s·ª≠ d·ª•ng context
            system_content = f"""IMPORTANT: You must ONLY use information from the provided context.
            If the context doesn't contain the answer, admit that you don't have the information.
            Never make up facts or use knowledge outside the context.
            DO NOT include any citations or sources in your response.
            
            {self.prompts[lang]['system']}"""

            messages = [
                HumanMessage(content=system_content),
                HumanMessage(content=str(question))
            ]

            # Collect entire response first
            full_response = ""
            sources = []
            async for chunk in self.llm.astream(messages):
                if hasattr(chunk, 'content'):
                    full_response += chunk.content
                if hasattr(chunk, 'document'):
                    # Extract metadata for citations
                    if 'metadata' in chunk.document:
                        sources.append(chunk.document.metadata)

            # Process complete response by paragraphs and sentences
            paragraphs = full_response.split('\n\n')
            
            # Check if response indicates no information available
            no_info_indicators = [
                "The provided text does not contain information",
                "I don't have information",
                "I don't have that information",
                "T√¥i kh√¥ng c√≥ th√¥ng tin",
                "VƒÉn b·∫£n kh√¥ng ch·ª©a th√¥ng tin"
            ]
            
            response_text = "\n\n".join(paragraphs)
            if any(indicator in response_text for indicator in no_info_indicators):
                # If no information, just yield the response without sources
                yield response_text
                return

            # If there is information, continue with normal processing
            for i, paragraph in enumerate(paragraphs):
                if not paragraph.strip():
                    continue
                    
                # Ensure paragraph ends with proper punctuation
                paragraph = paragraph.strip()
                if not paragraph[-1] in ('.', '!', '?'):
                    paragraph += '.'

                # Stream sentences within paragraph
                sentences = [s.strip() for s in paragraph.split('. ')]
                for j, sentence in enumerate(sentences):
                    if sentence:
                        # Add proper punctuation and spacing
                        if j < len(sentences) - 1:
                            yield sentence + '. '
                        else:
                            # Last sentence of paragraph
                            if not sentence[-1] in ('.', '!', '?'):
                                yield sentence + '. '
                            else:
                                yield sentence + ' '
                        await asyncio.sleep(0.1)
                
                # Add newline between paragraphs, except for last paragraph
                if i < len(paragraphs) - 1:
                    yield '\n'

            # Remove citation generation logic
            yield response_text

        except Exception as e:
            error_msg = "ƒê√£ x·∫£y ra l·ªói." if lang == 'vi' else "An error occurred."
            logger.error(f"Error in stream_response: {str(e)}")
            yield f"{error_msg}: {str(e)}"

    async def stream_chat(self, messages: List[BaseMessage]) -> AsyncGenerator[Any, None]:
        """Stream chat response with proper handling of context"""
        try:
            async for chunk in self.llm.astream(messages):
                if hasattr(chunk, 'content'):
                    # Return t·ª´ng chunk nh·ªè ƒë·ªÉ x·ª≠ l√Ω ·ªü test.py
                    yield chunk

        except Exception as e:
            logger.error(f"Error in stream_chat: {str(e)}")
            raise

if __name__ == "__main__":
    system_instruction = """
    As a human, I want to know the answer to the following question:
    Context: {context}
    Question: {question}
    """
    llm = GeminiModel(system_instruction=system_instruction)
    llm.create_stuff_documents_chain()